{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-12-12T02:22:53.171961Z","iopub.status.busy":"2021-12-12T02:22:53.171044Z","iopub.status.idle":"2021-12-12T02:22:53.313405Z","shell.execute_reply":"2021-12-12T02:22:53.31189Z","shell.execute_reply.started":"2021-12-12T02:22:53.171898Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","#--------------------------------------\n","#\n","#    Read manually preprocessed data\n","#\n","#--------------------------------------\n","\n","def saveDataFrame(path, df):\n","    print(\"Saving CSV:\", path)\n","    df.to_csv(path, encoding='utf-8-sig', index=False, sep=\";\")\n","\n","def readDataFrame(path):\n","    print(\"Reading CSV:\", path)\n","    return pd.read_csv(path, encoding='utf-8-sig', sep=\";\")  \n","\n","raw_data = readDataFrame(r'../input/ekr-docs/ekr_135_full_manually.csv')\n","\n","\n","print(raw_data.describe())\n","print('---------------------------------------')\n","print(raw_data['ocr_text'][0][:500])"]},{"cell_type":"markdown","metadata":{},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T02:22:53.316495Z","iopub.status.busy":"2021-12-12T02:22:53.315966Z","iopub.status.idle":"2021-12-12T02:22:53.488865Z","shell.execute_reply":"2021-12-12T02:22:53.487924Z","shell.execute_reply.started":"2021-12-12T02:22:53.316444Z"},"trusted":true},"outputs":[],"source":["#--------------------------------------\n","#\n","#    Convert sequences to NER format: [(tag, token), (tag, token), ...]\n","#\n","#--------------------------------------\n","\n","O_tag = 'O'\n","\n","class SequenceTagger():\n","\n","    \"\"\"\n","        Lowercarse and remove redundant whitespaces\n","    \"\"\"\n","    def clean(self, text): \n","        return ' '.join(str(text).lower().split())\n","    \n","    \n","    \"\"\"\n","        - Convert text and its tags to a list of tag-value pairs:\n","          'text ...', {'tag': 'value', ...}   =>   [('O', some), ('xtag', text), ('ytag', value), ...]\n","        - Text should contains tag_values exacatly --- TODO: use Levenshtein distance insted of exact match!\n","        - Calculate O tag values\n","        - Also tokenize text by whitespaces\n","    \"\"\"\n","    def tag_sequence(self, text, tag_values, unk_value='-'):\n","        \n","        text = self.clean(text)\n","        \n","        # calculate positions (charcter indexes) of tags  from tex(tag, start, end)\n","        tag_indexes = []\n","        for tag, value in tag_values.items():\n","            if value != unk_value:\n","                value = self.clean(value)\n","                start = text.find(value)\n","                if start != -1:\n","                    tag_indexes.append((tag, start, start+len(value)))\n","                else:\n","                    print('WARNING: no tag \"'+tag+'\" found with value \"'+value+'\"')\n","        \n","        # sort tags by appearence in text\n","        tag_indexes.sort(key=lambda val: val[1])\n","        \n","        tag_value_results = []\n","        # helper: add tag to results if its value is not empty\n","        def append(tag, value):\n","            if value:\n","                tag_value_results.append((tag, value))\n","                \n","        # create tag-value pairs by extracting values from text with indexes\n","        for i in range(len(tag_indexes)):\n","            \n","            tag, start, end = tag_indexes[i]\n","            \n","            # add O text before the first tag\n","            if i == 0:\n","                append(O_tag, text[0 : start])\n","                \n","            # add current tag\n","            append(tag, text[start : end])\n","                \n","            # add O text between current and next tag (or after the last tag)\n","            if i == len(tag_indexes)-1:\n","                next_start = None\n","            else:\n","                _, next_start, _ = tag_indexes[i+1]\n","            append(O_tag, text[end : next_start])\n","            \n","        return tag_value_results #, tag_value_results # return with whole sequences\n","\n","\n","    def tag_df(self, df, text_col, cols2tags):\n","        def apply_seq_tagging(df_row):\n","            tag_values = { tag: df_row[col] for col, tag in cols2tags.items() }\n","            return self.tag_sequence(df_row[text_col], tag_values)\n","        \n","        return df.apply(apply_seq_tagging, axis=1).values\n","\n","    def tag(self, docs):\n","        def apply_seq_tagging(doc):\n","            tag_values = doc['tags']\n","            return self.tag_sequence(doc['text'], tag_values)\n","        \n","        return [apply_seq_tagging(doc) for doc in docs]\n","    \n","\n","cols2tags = {\n","    'Ajánlatkérő adószáma': 'A_ADO', \n","    'Vezető ajánlattevő adószáma': 'V_ADO',\n","    'Ajánlatkérő cím': 'A_CIM', \n","    'Vezető ajánlattevő cím': 'V_CIM', \n","    'Ajánlatkérő megnevezése': 'A_NEV',\n","    'Vezető ajánlattevő megnevezése': 'V_NEV',\n","    'Szerződött bruttó ellenérték (összeg/keretösszeg)': 'BR_AR',\n","    'Szerződött nettó ellenérték (összeg/keretösszeg)': 'N_AR'\n","}\n","    \n","sequence_tagger = SequenceTagger()\n","tagged_docs = sequence_tagger.tag_df(raw_data, 'ocr_text', cols2tags)\n","\n","\n","print(tagged_docs[0][0:5]) # first doc first 5 tags"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T02:22:53.491154Z","iopub.status.busy":"2021-12-12T02:22:53.490807Z","iopub.status.idle":"2021-12-12T02:22:58.657793Z","shell.execute_reply":"2021-12-12T02:22:58.656912Z","shell.execute_reply.started":"2021-12-12T02:22:53.491091Z"},"trusted":true},"outputs":[],"source":["import re\n","\n","class PreservingTokenizer():\n","    \n","    \"\"\"\n","        Tokenize text values of a doc while preservering position informations\n","        - [(tag, value), ...] => [(token, token_index, tag), ...]\n","        - May split words to subwords => token_index contains the original word index\n","    \"\"\"\n","    def tokenize_doc(self, doc):\n","        tokenized_doc = []\n","        word_counter = 0\n","        for tag, value in doc:\n","            words = value.split()\n","            # making every a-Z and 1-9 standalone\n","            subwords = [(subword, word_counter+wi) for wi, word in enumerate(words) \\\n","                                                    for subword in re.sub(r'([^\\w])', r' \\1 ', word).split()]\n","                \n","            if tag == O_tag:\n","                for subword, wi in subwords:\n","                    tokenized_doc.append((subword, wi, tag))    \n","            else:\n","                subword, wi = subwords[0]\n","                tokenized_doc.append((subword, wi, 'B-'+tag))\n","                for subword, wi in subwords[1:]:\n","                    tokenized_doc.append((subword, wi, 'I-'+tag))\n","                    \n","            word_counter += len(words)\n","                    \n","        return tokenized_doc\n","        \n","        \n","    def tokenize_docs(self, docs):\n","        return [self.tokenize_doc(doc) for doc in docs]\n","    \n","    \"\"\"\n","        Detokenize (join) text values of a doc while preservering position informations\n","        - [(token, token_index, tag), ...] => [(tag, value), ...]\n","        - Concatenate splitted words by token_index\n","        - Handle prediction error for in-tag cases, e.g.: B I O I => B I I I\n","    \"\"\"\n","    def detokenize_doc(self, doc, max_error_distance=2):  # doc: [(subword, wi, tag), ...] # todo: handle removed token\n","        tag_values = []\n","        last_index = -1\n","        for i, item in enumerate(doc):\n","            subword, wi, tag = item\n","            tag = tag if tag == O_tag else tag[2:]\n","            \n","            if not tag_values:\n","                tag_values.append((tag, subword))\n","            else:\n","                last_tag, last_value = tag_values[-1]\n","                sep = '' if last_index == wi else ' '\n","                if last_tag == tag or \\\n","                (   # correct tag if the following tag is the same as last\n","                    last_tag != O_tag and \\\n","                    any([last_tag == next_tag for _, _, next_tag in doc[i:min(i+max_error_distance, len(doc))]])\n","                ):\n","                    tag_values[-1] = (last_tag, last_value +sep+ subword)\n","                else:\n","                    tag_values.append((tag, subword))\n","                \n","            last_index = wi\n","            \n","        return tag_values\n","    \n","    def detokenize_docs(self, docs):\n","        return [self.detokenize_doc(doc) for doc in docs]\n","\n","\n","\n","preserving_tokenizer = PreservingTokenizer()\n","tokenized_docs = preserving_tokenizer.tokenize_docs(tagged_docs)\n","\n","\n","print(tagged_docs[0][0:5]) # first doc first 5 tags\n","print(tokenized_docs[0][0:15]) # first doc first 15 tags\n","print(preserving_tokenizer.detokenize_docs(tokenized_docs)[0][0:5])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T02:22:58.660781Z","iopub.status.busy":"2021-12-12T02:22:58.660447Z","iopub.status.idle":"2021-12-12T02:25:41.786002Z","shell.execute_reply":"2021-12-12T02:25:41.785075Z","shell.execute_reply.started":"2021-12-12T02:22:58.660736Z"},"trusted":true},"outputs":[],"source":["#--------------------------------------\n","#\n","#    Normalize data\n","#\n","#--------------------------------------\n","\n","# Todo: use unk value? E.g. use only words contained by min 5% of every doc \n","\n","import unicodedata\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import SnowballStemmer\n","nltk.download('stopwords')\n","\n","\n","# Turn a Unicode string to plain ASCII, thanks to # https://stackoverflow.com/a/518232/2809427\n","def unicodeToAscii(s):\n","    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n","\n","\n","class FeaturePreprocessor():\n","    def __init__(self, lang = 'hungarian'):\n","        self.stopwords = stopwords.words(lang)\n","        self.stemmer = SnowballStemmer(lang)\n","\n","    \"\"\"\n","        Lowercarse and stemm\n","    \"\"\"    \n","    def normalizeWord(self, word):\n","        lowered = str(word).casefold()\n","        stemmed = self.stemmer.stem(lowered)\n","        \n","        lowered = unicodeToAscii(lowered)\n","        stemmed = unicodeToAscii(stemmed)\n","        \n","        return lowered, stemmed\n","    \n","    \"\"\"\n","        Create feature dictionary for word at doc[index+shift]\n","    \"\"\"\n","    def word_features(self, doc, index, shift=0): # todo reduce run time\n","        index = index + shift\n","        word = doc[index][0]\n","        lowered, stemmed = self.normalizeWord(word)\n","\n","        prefix = f'{shift}:' if shift < 0 else (f'+{shift}:' if shift > 0 else '')\n","        features = {\n","            # todo: isupper, istitle\n","            'bias': 1.0,\n","            f'{prefix}stem': stemmed,\n","            f'{prefix}[:3]': lowered[:3],\n","            f'{prefix}[:2]': lowered[:2],\n","            f'{prefix}[-3:]': lowered[-3:],\n","            f'{prefix}[-2:]': lowered[-2:],\n","            f'{prefix}isdigit': lowered.isdigit(),\n","            f'{prefix}isalpha': lowered.isalpha(),\n","            f'{prefix}isalnum': lowered.isalnum(),\n","            f'{prefix}isstop': lowered in self.stopwords\n","        }\n","        if index == 0:\n","            features[f'{prefix}BOS'] = True\n","        elif index == len(doc)-1:\n","            features[f'{prefix}EOS'] = True\n","            \n","        return features\n","    \n","    \"\"\"\n","        Create n-gram like features for word at doc[index]\n","        - n = 2 * n_step + 1\n","    \"\"\"\n","    def n_gram(self, doc, index, n_step):\n","        features = {}\n","        for shift in range(-n_step, n_step+1):\n","            if index+shift >= 0 and index+shift < len(doc):\n","                features.update(self.word_features(doc, index, shift))\n","\n","        return (features, doc[index]) \n","    \n","    \n","    def transform(self, docs, n_step=2):\n","        return [[self.n_gram(doc, index, n_step) for index in range(len(doc))] for doc in docs]\n","    \n","\n","\n","feature_preprocessor = FeaturePreprocessor()\n","prepared_docs = feature_preprocessor.transform(tokenized_docs)\n","\n","\n","print(' '.join(np.array(tokenized_docs[0])[:100,0]))\n","print('---------------------------------------')\n","print(np.array(prepared_docs[0])[2]) # first doc third feature\n","print('---------------------------------------')\n","print(' '.join(np.array(tokenized_docs[0])[:5,2])) # first doc first 5 tags"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T02:25:41.7886Z","iopub.status.busy":"2021-12-12T02:25:41.787912Z","iopub.status.idle":"2021-12-12T02:25:41.986562Z","shell.execute_reply":"2021-12-12T02:25:41.985332Z","shell.execute_reply.started":"2021-12-12T02:25:41.788566Z"},"trusted":true},"outputs":[],"source":["#--------------------------------------\n","#\n","#    Split doc into equal-length windows\n","#\n","#--------------------------------------\n","\n","\n","# TODO: use the longest tag value for calc the window size?\n","class SequenceSplitter():\n","    \"\"\"\n","        Split length evenly into sublength\n","    \"\"\" \n","    def split_length_evenly(self, total_len, max_sublength):\n","        # calculate split lengths\n","        quotient = int(total_len / max_sublength) \n","        remainder = total_len % max_sublength\n","        if remainder == 0:\n","            return np.array([max_sublength] * quotient)\n","        lens = np.array([max_sublength] * quotient + [remainder])\n","        \n","        # optimize length to have the smallest diff between min and max\n","        min_len, max_len = lens.min(), lens.max()\n","        while(max_len - min_len >= 2):\n","            lens[lens.argmin()] = min_len + 1\n","            lens[lens.argmax()] = max_len - 1\n","            min_len, max_len = lens.min(), lens.max()\n","       \n","        return lens\n","    \n","    \"\"\"\n","        Get starting indexes of sequence windows\n","    \"\"\"\n","    def window_indexes(self, total_size, window_size, min_overlap_size):\n","        total_len = total_size\n","        # window = section + overlapping\n","        # total_len = section1 + section2 +...+ last_section\n","        \n","        # last section have full window size since it can not overlap with the following section\n","        last_section_len = window_size\n","        # other sections may have various sizes, overlapping should \"pad\" them to window size\n","        sections_total_len = total_len - last_section_len\n","        max_section_len = window_size - min_overlap_size\n","        \n","        # calculate sections' various lengths\n","        section_lens = self.split_length_evenly(sections_total_len, max_section_len)\n","        section_lens = np.append(section_lens, last_section_len)\n","        \n","        # calculate sections' (or windows') starting indexes\n","        window_indexes = section_lens.cumsum() - section_lens\n","        \n","        return window_indexes\n","    \n","    \"\"\"\n","        Split tokens and tags into overlapping windows\n","    \"\"\"\n","    def n_paragraph(self, doc, window_size, min_overlap_size):\n","        \n","        total_len = len(doc)\n","        window_indexes = self.window_indexes(total_len, window_size, min_overlap_size)\n","        \n","        return [doc[i : i+window_size] for i in window_indexes]\n","\n","    \"\"\"\n","        Split every doc into overlapping windows with equal lengths\n","    \"\"\"\n","    def split_docs(self, docs, window_size=150, min_overlap_size=50):\n","        splitted_docs = []\n","        for doc_i, doc in enumerate(docs):\n","            for window in self.n_paragraph(doc, window_size, min_overlap_size):\n","                splitted_docs.append((doc_i, window))\n","        return splitted_docs\n","\n","    \"\"\"\n","        Concat overlapping windows into original format docs\n","        - Handle overlapping tags\n","    \"\"\"\n","    def concat_doc(self, doc, window_size=150, min_overlap_size=50): # doc: [(subword, wi, tag), ...]\n","        section_size = window_size - min_overlap_size\n","        concated_doc = []\n","        next_token_i = -1\n","        for window_i, window in enumerate(doc):\n","            for token_i, item in enumerate(window):\n","                _, wi, tag = item\n","                # if the previous window's overlap already added this token\n","                if token_i <= next_token_i:\n","                    continue\n","                # if in overlap and O tag, go to the next window (except if last window)\n","                if token_i > section_size and tag == O_tag and not window_i == len(doc)-1:\n","                    break\n","                # else add new token item\n","                concated_doc.append(item)\n","                next_token_index = wi\n","                \n","        return concated_doc\n","    \n","    \"\"\"\n","        Concat every windows into their original doc\n","    \"\"\"\n","    def concat_docs(self, docs, window_size=150, min_overlap_size=50):\n","        splitted_docs = [[]]\n","        last_doc_i = 0\n","        for doc_i, window in docs:\n","            if doc_i == last_doc_i:\n","                splitted_docs[-1].append(window)\n","            else:\n","                splitted_docs.append([window])\n","                last_doc_i = doc_i\n","                \n","        return [self.concat_doc(doc, window_size, min_overlap_size) for doc in splitted_docs]\n","                \n","                   \n","    \n","sequence_splitter = SequenceSplitter()\n","window_size = 150\n","overlap_size = 50\n","splitted_docs = sequence_splitter.split_docs(prepared_docs, window_size, overlap_size)\n","\n","\n","print(splitted_docs[0][1][1]) # first doc, second window, second token"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T02:25:41.988434Z","iopub.status.busy":"2021-12-12T02:25:41.988201Z","iopub.status.idle":"2021-12-12T02:25:42.926876Z","shell.execute_reply":"2021-12-12T02:25:42.925848Z","shell.execute_reply.started":"2021-12-12T02:25:41.988405Z"},"trusted":true},"outputs":[],"source":["X = [[item[0] for item in window] for _, window in splitted_docs]\n","y = [[item[1][2] for item in window] for _, window in splitted_docs]\n","\n","print(np.array(X).shape)\n","print(np.array(y).shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T02:25:42.92862Z","iopub.status.busy":"2021-12-12T02:25:42.928408Z","iopub.status.idle":"2021-12-12T02:25:54.582486Z","shell.execute_reply":"2021-12-12T02:25:54.58138Z","shell.execute_reply.started":"2021-12-12T02:25:42.928592Z"},"trusted":true},"outputs":[],"source":["#!pip install sklearn_crfsuite"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T02:25:54.584449Z","iopub.status.busy":"2021-12-12T02:25:54.584201Z","iopub.status.idle":"2021-12-12T02:25:54.589679Z","shell.execute_reply":"2021-12-12T02:25:54.588848Z","shell.execute_reply.started":"2021-12-12T02:25:54.584415Z"},"trusted":true},"outputs":[],"source":["def flatten(l):\n","    return [item for sublist in l for item in sublist]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T02:25:54.591538Z","iopub.status.busy":"2021-12-12T02:25:54.591263Z"},"trusted":true},"outputs":[],"source":["#--------------------------------------\n","#\n","#    Train model\n","#\n","#--------------------------------------\n","\n","import sklearn\n","from sklearn.metrics import make_scorer\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import RandomizedSearchCV\n","\n","import sklearn_crfsuite\n","from sklearn_crfsuite import scorers\n","from sklearn_crfsuite import metrics\n","\n","crf = sklearn_crfsuite.CRF(\n","    all_possible_transitions=True\n",")\n","\n","params_space = {\n","    'algorithm': ['l2sgd', 'lbfgs'], # lbfgs, l2sgd\n","    #'c2': [0.9],\n","    #'c1': [0.2, 0.7], # csak lbfgs\n","    'max_iterations': [100, 200], #[700, 1000], \n","    #'num_memories': [6, 10], # csak lbfgs\n","}\n","\n","f1_scorer = make_scorer(metrics.flat_f1_score, average='weighted', labels=np.unique(flatten(y)))\n","\n","rs = RandomizedSearchCV(\n","    crf, params_space,\n","    cv=4,\n","    verbose=1,\n","    n_jobs=1,\n","    n_iter=10,\n","    scoring=f1_scorer\n",")\n","\n","rs.fit(X, y)\n","crf = rs.best_estimator_\n","\n","print('best params:', rs.best_params_)\n","print('best CV score:', rs.best_score_)\n","print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#--------------------------------------\n","#\n","#    Evaluate and save model\n","#\n","#--------------------------------------\n","\n","labels = list(crf.classes_)\n","\n","y_pred = crf.predict(X)\n","\n","print(classification_report(flatten(y), flatten(y_pred), labels=labels))\n","\n","labels.remove('O')\n","metrics.flat_f1_score(y, y_pred, average='weighted', labels=labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from joblib import dump, load\n","dump(crf, 'model.joblib')"]},{"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tagged_test = [[(O_tag, text)] for text in raw_data['ocr_text'].values]\n","\n","tokenized_test = preserving_tokenizer.tokenize_docs(tagged_test)\n","\n","print(tokenized_test[0][:15])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["prepared_test = feature_preprocessor.transform(tokenized_test)\n","\n","print(prepared_test[0][2])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["splitted_test = sequence_splitter.split_docs(prepared_test)\n","\n","print(splitted_test[0][1][1])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X_test = [[item[0] for item in window] for _, window in splitted_test]\n","\n","print(np.array(X).shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["loaded_crf = load('model.joblib')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_pred = loaded_crf.predict(X_test)\n","\n","print(test_pred[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def merge_pred_with_docs(pred, docs):\n","    return [\n","        (\n","            docs[i][0], \n","            [\n","                (\n","                    docs[i][1][ii][1][0], \n","                    docs[i][1][ii][1][1], \n","                    pred[i][ii]\n","                ) \n","                for ii in range(len(docs[0]))\n","            ]\n","        ) \n","        for i in range(len(docs))\n","    ]\n","\n","merged_pred = merge_pred_with_docs(test_pred, splitted_test)\n","\n","print(merged_pred[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["concat_pred = sequence_splitter.concat_docs(merged_pred)\n","\n","print(concat_pred[0][:15])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["detokenized_pred = preserving_tokenizer.detokenize_docs(concat_pred)\n","\n","print(detokenized_pred[0][:5])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def extract_results(pred):\n","    results = []\n","    for doc in pred:\n","        tag_values = {}\n","        for tag, value in doc:\n","            if tag != O_tag and (tag not in tag_values or len(tag_values[tag]) < len(value)):\n","                tag_values[tag] = value\n","        results.append(tag_values)\n","        \n","    return results\n","\n","  \n","results = extract_results(detokenized_pred)\n","    \n","print(results[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def init():\n","    model = load('model.joblib')\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def predict(texts, model):\n","    tagged_test = [[(O_tag, text)] for text in texts]\n","    tokenized_test = preserving_tokenizer.tokenize_docs(tagged_test)\n","    prepared_test = FeaturePreprocessor().transform(tokenized_test)\n","    splitted_test = sequence_splitter.split_docs(prepared_test)\n","    X_test = [[item[0] for item in window] for _, window in splitted_test]\n","    \n","    test_pred = model.predict(X_test)\n","\n","    merged_pred = merge_pred_with_docs(test_pred, splitted_test)\n","    concat_pred = sequence_splitter.concat_docs(merged_pred)\n","    detokenized_pred = preserving_tokenizer.detokenize_docs(concat_pred)\n","    results = extract_results(detokenized_pred)\n","    return results"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
